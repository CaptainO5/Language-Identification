{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LangugageIdentification_CRF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Os-jQahnOzVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import lxml.etree as et\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.model_selection import train_test_split as tts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2aqGGsFTbGr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ngrams(string, N=5):\n",
        "  ngrams = []\n",
        "  for n in range(1, N + 1):\n",
        "    for i in range(len(string)-n+1):\n",
        "        ngrams.append(string[i:i+n])\n",
        "  s = \"\"\n",
        "  for w in ngrams:\n",
        "    s += w + \" \";\n",
        "  return s.strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EneQIfgTqW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def makeNgramData(filename):\n",
        "  lan = list()\n",
        "  with open(filename) as f:\n",
        "    for line in f:\n",
        "      lan.append(line.strip())\n",
        "  lan = pd.Series(lan).apply(ngrams)\n",
        "  data = {\"word\": lan, \"label\": 1}\n",
        "  return pd.DataFrame(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEXZ8n7WTfxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainModels(lanFile):\n",
        "  filename = \"./eng2.txt\"\n",
        "  eng = list()\n",
        "  with open(filename) as f:\n",
        "    for line in f:\n",
        "      eng.append(line.strip())\n",
        "  data = {\"word\":eng, \"label\": 0}\n",
        "  data = pd.DataFrame(data)\n",
        "  # print(data.head(20))\n",
        "\n",
        "  data = data.append(makeNgramData(lanFile), ignore_index=True)\n",
        "  \n",
        "  # Shuffling rows\n",
        "  data = data.sample(frac=1.0, random_state=5).reset_index(drop=True)\n",
        "\n",
        "  \n",
        "  data_x = data['word']\n",
        "  data_y = data['label']\n",
        "  \n",
        "  # # Testing with 20% data\n",
        "  # x_train, x_test, y_train, y_test = tts(data_x, data_y, test_size=.2, shuffle=True)\n",
        "\n",
        "  # cv = CountVectorizer()\n",
        "  # x_train = cv.fit_transform(x_train)\n",
        "  # tt = TfidfTransformer()\n",
        "  # x_train = TfidfTransformer().fit_transform(x_train)\n",
        "\n",
        "  # models = [MultinomialNB(), LogisticRegression(), MLPClassifier(max_iter=10)]\n",
        "  # f1_scores = []\n",
        "  # for model in models:\n",
        "  #   # print(model, \"\\n\")\n",
        "  #   model = model.fit(x_train, y_train)\n",
        "  #   y_pred = model.predict(tt.transform(cv.transform(x_test)))\n",
        "  #   f1_scores.append(metrics.f1_score(y_test, y_pred, labels=['eng', lanFile[:3]]))\n",
        "  #   # print(metrics.classification_report(y_test, y_pred, target_names=['English', 'Telugu']))\n",
        "  # print(lanFile[:3].upper(), \"F-scores\")\n",
        "  # print(f1_scores)\n",
        "\n",
        "  # Fitting on total data.\n",
        "  cv = CountVectorizer()\n",
        "  tt = TfidfTransformer()\n",
        "  data_x = cv.fit_transform(data_x)\n",
        "  data_x = tt.fit_transform(data_x)\n",
        "  models = [MultinomialNB().fit(data_x, data_y)]#, LogisticRegression().fit(data_x, data_y), MLPClassifier(n_iter_no_change=5).fit(data_x, data_y)]\n",
        "  return models, cv, tt #, f1_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyBZF0xx9zM-",
        "colab_type": "code",
        "outputId": "1020f48a-ce59-41b9-f37b-219d2af3595b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "%%time\n",
        "bin_models = dict()\n",
        "bin_models[\"te_en_\"] = trainModels(\"telugu.txt\")\n",
        "bin_models[\"ml_en_\"] = trainModels(\"malayalamW.txt\")\n",
        "bin_models[\"bn_en_\"] = trainModels(\"bengaliW.txt\")\n",
        "bin_models[\"gu_en_\"] = trainModels(\"gujaratiW.txt\")\n",
        "bin_models[\"hi_en_\"] = trainModels(\"hindiW.txt\")\n",
        "bin_models[\"kn_en_\"] = trainModels(\"kannadaW.txt\")\n",
        "bin_models[\"mr_en_\"] = trainModels(\"maratiW.txt\")\n",
        "bin_models[\"ta_en_\"] = trainModels(\"tamil.txt\")"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 9.4 s, sys: 69.8 ms, total: 9.47 s\n",
            "Wall time: 9.49 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsWwe-LxWG1f",
        "colab_type": "text"
      },
      "source": [
        "Sentence level classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeu1AmBJSSMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getC(path):\n",
        "  '''\n",
        "  Return sequences from the XML file, un-cleaned \n",
        "  '''\n",
        "  tree = et.parse(path, parser=et.XMLParser(recover=True))\n",
        "  root = tree.getroot()\n",
        "  data = []\n",
        "  for i in root.getchildren():\n",
        "    data.append(i.text.strip())\n",
        "  data = [i.split(' ') for i in data]\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zZTIUftE8H7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string, numpy as np\n",
        "\n",
        "langs = ['bn', 'en', 'gu', 'hi', 'kn', 'ml', 'mr', 'ta', 'te']\n",
        "\n",
        "def getSeqNLbl(ipfile, annotfile):\n",
        "  '''\n",
        "  Returns sequences and corresponding Labels without redundant words from the given XML files\n",
        "  '''\n",
        "\n",
        "  punctuations = [i for i in string.punctuation]\n",
        "\n",
        "  annot_ = getC(annotfile)\n",
        "  ip_ = getC(ipfile)\n",
        "\n",
        "  _seq= []\n",
        "  _labels = []\n",
        "  for i in range(len(annot_)):\n",
        "    tl = [] # Temp list for labels\n",
        "    ts = [] # Temp list for sequences\n",
        "\n",
        "    if len(annot_[i]) == len(ip_[i]):\n",
        "      for l, word in enumerate(ip_[i]):\n",
        "        if annot_[i][l] != 'X' and np.all([p not in word for p in punctuations]):\n",
        "          tl.append(annot_[i][l])\n",
        "          ts.append(word)\n",
        "      _labels.append(tl)\n",
        "      _seq.append(' '.join(ts))\n",
        "\n",
        "  # remove labels, sequences without language tags or with more than two language tags\n",
        "  e = enumerate(_labels)\n",
        "  for i, j in e:\n",
        "    lan = set()\n",
        "    for l in j:\n",
        "      if l in langs:\n",
        "        lan.add(l)\n",
        "    if len(lan) < 1 or len(lan) > 2 or (len(lan) == 2 and 'en' not in lan):\n",
        "      _labels.pop(i)\n",
        "      _seq.pop(i)\n",
        "  \n",
        "  return _seq, _labels\n",
        "\n",
        "def getLangLbl(lbls):\n",
        "  lbl =[]\n",
        "  for i in lbls:\n",
        "    temp = set()\n",
        "    for j in i:\n",
        "      if j in langs:\n",
        "        temp.add(j)\n",
        "    temp.add('en')\n",
        "    temp = list(temp)\n",
        "    temp.remove('en')\n",
        "    temp.append('en')\n",
        "    lbl.append('_'.join(temp) + '_')\n",
        "  return lbl\n",
        "\n",
        "\n",
        "# Retrieving data\n",
        "train_seq, train_labels = getSeqNLbl(\"InputTraining.txt\", \"AnnotationTraining.txt\")\n",
        "train_lang_lbls = getLangLbl(train_labels)\n",
        "\n",
        "test_seq, test_labels = getSeqNLbl(\"InputTesting.txt\", \"AnnotationTesting.txt\")\n",
        "test_lang_lbls = getLangLbl(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TdVCoPIKkaW",
        "colab_type": "code",
        "outputId": "c66b0b81-48a3-4afa-c5f9-0795d07b02d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "sent_labels = {j:i for i,j in enumerate(set(train_lang_lbls))} \n",
        "\n",
        "# Copy of training and testing data\n",
        "X_train, y_train = train_seq[:], train_lang_lbls[:]\n",
        "X_test, y_test = test_seq[:], test_lang_lbls[:]\n",
        "\n",
        "# Transforming \n",
        "count_vect = CountVectorizer(ngram_range=(1, 5))\n",
        "tf_idf_transformer = TfidfTransformer()\n",
        "\n",
        "X_train_trans = tf_idf_transformer.fit_transform(count_vect.fit_transform(X_train))\n",
        "\n",
        "# Training and testing MLP model\n",
        "model = MLPClassifier(hidden_layer_sizes=(120)) \n",
        "print(\"MLP CLassification report\\n\")\n",
        "model = model.fit(X_train_trans, y_train)\n",
        "y_pred = model.predict(tf_idf_transformer.transform(count_vect.transform(X_test)))\n",
        "print(metrics.classification_report(y_test, y_pred))"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP CLassification report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      bn_en_       0.72      0.70      0.71       139\n",
            "         en_       0.47      0.94      0.62       148\n",
            "      gu_en_       0.12      0.06      0.08        16\n",
            "      hi_en_       0.92      0.61      0.73       223\n",
            "      kn_en_       0.89      0.65      0.75       101\n",
            "      ml_en_       0.80      0.20      0.32        20\n",
            "      mr_en_       0.73      0.93      0.82        29\n",
            "      ta_en_       0.55      0.96      0.70        24\n",
            "      te_en_       0.85      0.36      0.50        78\n",
            "\n",
            "    accuracy                           0.67       778\n",
            "   macro avg       0.67      0.60      0.58       778\n",
            "weighted avg       0.75      0.67      0.67       778\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgU8_y8CU6kT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num2snt_lbl = dict(enumerate(set(train_lang_lbls)))\n",
        "\n",
        "X_test_seq = [i.split() for i in X_test]\n",
        "y_test_lbls = test_labels[:]\n",
        "\n",
        "y_pred_lbls = []\n",
        "up = []\n",
        "for i in range(len(y_pred)):\n",
        "  snt_lbl = y_pred[i]\n",
        "  if len(y_test_lbls[i]) == len(X_test_seq[i]):\n",
        "    if snt_lbl in bin_models:\n",
        "      modelI = bin_models[snt_lbl]\n",
        "      model = modelI[0][0]\n",
        "      pred_seq = model.predict(modelI[2].transform(modelI[1].transform([ngrams(w) for w in X_test_seq[i]])))\n",
        "      pred_seq = [snt_lbl[:2] if i == 1 else 'en' for i in pred_seq]\n",
        "    else:\n",
        "      # For 'en_'\n",
        "      pred_seq = ['en' for i in range(len(X_test_seq[i]))]\n",
        "    y_pred_lbls.append(pred_seq)\n",
        "  else:\n",
        "    up.append(i)\n",
        "for i in up: # len(up) is equal to 1\n",
        "  y_test_lbls.pop(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnnYM7VpZ1eE",
        "colab_type": "code",
        "outputId": "128380c5-415c-42e2-ae4a-a9b0822ec62a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "lng2num = {j:i for i, j in enumerate(langs)}\n",
        "num2lng = dict(enumerate(langs))\n",
        "\n",
        "# Flattening for calculating metrics\n",
        "y_test_lbls_flat = []\n",
        "for lngs in y_test_lbls:\n",
        "  y_test_lbls_flat += lngs\n",
        "\n",
        "y_pred_lbls_flat = []\n",
        "for lngs in y_pred_lbls:\n",
        "  y_pred_lbls_flat += lngs\n",
        "\n",
        "# print(metrics.f1_score(y_test_lbls_flat, y_pred_lbls_flat, average=None, labels=langs))\n",
        "\n",
        "print(metrics.classification_report(y_test_lbls_flat, y_pred_lbls_flat, labels=langs))"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          bn       0.84      0.76      0.80      1363\n",
            "          en       0.66      0.96      0.79      3970\n",
            "          gu       0.41      0.10      0.16       185\n",
            "          hi       0.90      0.69      0.78      1583\n",
            "          kn       0.87      0.76      0.82       595\n",
            "          ml       0.94      0.46      0.62       231\n",
            "          mr       0.82      0.93      0.87       453\n",
            "          ta       0.91      0.65      0.76       543\n",
            "          te       0.83      0.32      0.46       529\n",
            "\n",
            "   micro avg       0.75      0.79      0.77      9452\n",
            "   macro avg       0.80      0.63      0.67      9452\n",
            "weighted avg       0.77      0.79      0.76      9452\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21NU3Vi7Mni9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install sklearn_crfsuite\n",
        "import sklearn_crfsuite"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV8cVvUzPCia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predicting labels using Binary classifiers for the Training sentences\n",
        "\n",
        "X_train_seq = [i.split() for i in train_seq]\n",
        "y_train_lbls = train_labels[:]\n",
        "\n",
        "y_seq_lbls = []\n",
        "up = []\n",
        "for i in range(len(y_train)):\n",
        "  snt_lbl = y_train[i]\n",
        "  if len(y_train_lbls[i]) == len(X_train_seq[i]):\n",
        "    if snt_lbl in bin_models:\n",
        "      modelI = bin_models[snt_lbl]\n",
        "      model = modelI[0][0]\n",
        "      pred_seq = model.predict(modelI[2].transform(modelI[1].transform([ngrams(w) for w in X_train_seq[i]])))\n",
        "      pred_seq = [snt_lbl[:2] if i == 1 else 'en' for i in pred_seq]\n",
        "    else:\n",
        "      # For 'en_'\n",
        "      pred_seq = ['en' for i in range(len(X_train_seq[i]))]\n",
        "    y_seq_lbls.append(pred_seq)\n",
        "  else:\n",
        "    up.append(i)\n",
        "for i in up:\n",
        "  y_train_lbls.pop(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSdvGUgMRkUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word2features(sent, i):\n",
        "  word = sent[i]\n",
        "\n",
        "  features = {\n",
        "    'bias': 1.0,\n",
        "    'word.lower()': word.lower(),\n",
        "  }\n",
        "\n",
        "  if i > 0:\n",
        "    word1 = sent[i-1]\n",
        "    features.update({\n",
        "      '-1:word.lower()': word1.lower(),\n",
        "    })\n",
        "  else:\n",
        "    features['BOS'] = True\n",
        "\n",
        "  # if i > 1:\n",
        "  #   word2 = sent[i-2]\n",
        "  #   features.update({\n",
        "  #     '-2:word.lower()': word2.lower(),\n",
        "  #   })\n",
        "\n",
        "  if i < len(sent)-1:\n",
        "    word1 = sent[i+1]\n",
        "    features.update({\n",
        "      '+1:word.lower()': word1.lower(),\n",
        "    })\n",
        "  else:\n",
        "    features['EOS'] = True\n",
        "\n",
        "  # if i < len(sent)-2:\n",
        "  #   word2 = sent[i+2]\n",
        "  #   features.update({\n",
        "  #     '+2:word.lower()': word2.lower()\n",
        "  #   })\n",
        "\n",
        "  return features\n",
        "\n",
        "def sent2features(sent):\n",
        "  return [word2features(sent, i) for i in range(len(sent))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjxQsVzJbSeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Taking predicted sequences as X and actual sequences as y for crf model training\n",
        "crfX_train = [sent2features(s) for s in y_seq_lbls]\n",
        "crfy_train = [[i.encode('ascii', 'ignore') for i in j] for j in y_train_lbls]\n",
        "\n",
        "crfX_test = [sent2features(s) for s in y_pred_lbls]\n",
        "crfy_test = y_test_lbls[:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "groJ6OLpbyPr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "outputId": "f5c2121f-e7dc-433e-d64a-677249308217"
      },
      "source": [
        "crf = sklearn_crfsuite.CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=100,all_possible_transitions=True)\n",
        "\n",
        "crf.fit(crfX_train, crfy_train)\n",
        "\n",
        "crf_pred = crf.predict(crfX_test)\n",
        "\n",
        "from sklearn_crfsuite import metrics as m\n",
        "print(m.flat_classification_report(crfy_test, crf_pred, digits=3, labels=langs))"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          bn      0.809     0.824     0.816      1363\n",
            "          en      0.687     0.917     0.785      3970\n",
            "          gu      0.355     0.119     0.178       185\n",
            "          hi      0.873     0.692     0.772      1583\n",
            "          kn      0.877     0.745     0.805       595\n",
            "          ml      0.926     0.489     0.640       231\n",
            "          mr      0.828     0.927     0.875       453\n",
            "          ta      0.799     0.818     0.808       543\n",
            "          te      0.683     0.406     0.509       529\n",
            "\n",
            "   micro avg      0.751     0.795     0.772      9452\n",
            "   macro avg      0.760     0.660     0.688      9452\n",
            "weighted avg      0.760     0.795     0.764      9452\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}